# Comparing-BART-and-BioBERT-for-Medical-Article-Summarization

This repository contains code for summarizing medical articles using two powerful models: BART (Bidirectional and AutoRegressive Transformers) and BioBERT (Biomedical Language Model).

## Overview

Summarizing medical articles is a crucial task in the field of healthcare and research. This project aims to explore the effectiveness of two state-of-the-art models, BART and BioBERT, for generating concise summaries of medical articles.

## Features

- Utilizes the BART model for medical article summarization
- Harnesses the power of BioBERT, a biomedical language model, for generating summaries
- Implements advanced natural language processing techniques for pre-processing and tokenization
- Provides a comparative analysis of the summarization results between BART and BioBERT
- Evaluates the quality of the generated summaries using established metrics
- Offers easy-to-use functions and example scripts for summarizing medical articles

## Prerequisites

- Python 3.x
- PyTorch
- Transformers library

## Contributing
Contributions to this project are welcome! If you have any ideas, suggestions, or bug fixes, please open an issue or submit a pull request.

## License
This project is licensed under the MIT License.

## Acknowledgments
We would like to express our gratitude to the creators and maintainers of BART and BioBERT, whose models and pre-trained weights have been instrumental in this project.

## References
For more information about BART and BioBERT, please refer to the following resources:

BART: https://arxiv.org/abs/1910.13461
BioBERT: https://academic.oup.com/bioinformatics/article/36/4/1234/5566506
